import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms, models
from torch.utils.data import DataLoader
import pickle
import json

# ğŸ›  Image Preprocessing
# Resize all input images to 224x224.
# You can change this size, but 224x224 gives a good balance
# between accuracy and performance in most models.

# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ­ÙˆÙŠÙ„Ø§Øª Ù„Ù„ØµÙˆØ± (ØªØµØºÙŠØ± + ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ Tensor)
transform = transforms.Compose([
    transforms.Resize((224, 224)),  # ØªÙˆØ­ÙŠØ¯ Ø­Ø¬Ù… Ø§Ù„ØµÙˆØ±
    transforms.ToTensor(),  # ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØ±Ø© Ø¥Ù„Ù‰ Tensor
    transforms.Normalize([0.5], [0.5])  # ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
])

# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Ø§Ù„Ù…Ø¬Ù„Ø¯
data_dir = "data"
dataset = datasets.ImageFolder(root=data_dir, transform=transform)
dataloader = DataLoader(dataset, batch_size=16, shuffle=True)

# Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù‚Ø§Ù…ÙˆØ³ Ù…Ù† Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„ÙØ¦Ø§Øª
class_mapping = {v: k for k, v in dataset.class_to_idx.items()}

# Ø­ÙØ¸ Ø§Ù„Ù‚Ø§Ù…ÙˆØ³ ÙÙŠ Ù…Ù„Ù JSON
with open("class_mapping.json", "w", encoding="utf-8") as f:
    json.dump(class_mapping, f, ensure_ascii=False, indent=4)

# Ø§Ø³ØªØ®Ø¯Ø§Ù… ResNet18 ÙƒÙ†Ù…ÙˆØ°Ø¬ Ø£Ø³Ø§Ø³ÙŠ
model = models.resnet18(pretrained=True)
num_ftrs = model.fc.in_features
model.fc = nn.Linear(num_ftrs, 2)  # ØªØµÙ†ÙŠÙ Ù„ÙØ¦ØªÙŠÙ†

# Ù†Ù‚Ù„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù„Ù‰ GPU Ø¥Ø°Ø§ ÙƒØ§Ù† Ù…ØªØ§Ø­Ù‹Ø§
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# Ø¥Ø¹Ø¯Ø§Ø¯ Ø¯Ø§Ù„Ø© Ø§Ù„ÙÙ‚Ø¯ ÙˆØ§Ù„Ù…ÙØ­Ø³Ù‘Ù†
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.0005)

# ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
epochs = 10
best_loss = float("inf")  # ØªØ¹ÙŠÙŠÙ† Ù‚ÙŠÙ…Ø© Ø¹Ø§Ù„ÙŠØ© ÙÙŠ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ©
best_model_path = "image_classifier.pkl"

for epoch in range(epochs):
    running_loss = 0.0
    for images, labels in dataloader:
        images, labels = images.to(device), labels.to(device)

        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()
    
    avg_loss = running_loss / len(dataloader)
    print(f"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}")

    # Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø£ÙØ¶Ù„ Ø­ØªÙ‰ Ø§Ù„Ø¢Ù†
    if avg_loss < best_loss:
        best_loss = avg_loss
        with open(best_model_path, "wb") as file:
            pickle.dump(model, file)
        print(f"ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø¬Ø¯ÙŠØ¯ Ù…Ø¹ Ø£Ù‚Ù„ Ø®Ø³Ø§Ø±Ø©: {best_loss:.4f}")

print("ØªÙ… Ø§Ù„Ø§Ù†ØªÙ‡Ø§Ø¡ Ù…Ù† Ø§Ù„ØªØ¯Ø±ÙŠØ¨!")
