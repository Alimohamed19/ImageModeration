import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms, models
from torch.utils.data import DataLoader
import pickle
import json
import os

# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ­ÙˆÙŠÙ„Ø§Øª Ù„Ù„ØµÙˆØ±
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize([0.5], [0.5])
])

# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
data_dir = "data"
dataset = datasets.ImageFolder(root=data_dir, transform=transform)
dataloader = DataLoader(dataset, batch_size=16, shuffle=True)

# Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù‚Ø§Ù…ÙˆØ³ Ù…Ù† Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„ÙØ¦Ø§Øª
class_mapping = {v: k for k, v in dataset.class_to_idx.items()}
with open("class_mapping.json", "w", encoding="utf-8") as f:
    json.dump(class_mapping, f, ensure_ascii=False, indent=4)

print("âœ… ØªÙ… Ø­ÙØ¸ Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„ÙƒÙ„Ø§Ø³Ø§Øª ÙÙŠ class_mapping.json")

# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ø³Ù… Ø§Ù„Ù…Ù„Ù Ù„Ù„Ù…ÙˆØ¯ÙŠÙ„
best_model_path = "image_classifier_best.pkl"

# Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ù†Ù…ÙˆØ°Ø¬ Ù…Ø­ÙÙˆØ¸
if os.path.exists(best_model_path):
    with open(best_model_path, "rb") as file:
        model = pickle.load(file)
    print("âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù‚Ø¯ÙŠÙ… Ø¨Ù†Ø¬Ø§Ø­!")
else:
    model = models.resnet18(pretrained=True)
    num_ftrs = model.fc.in_features
    model.fc = nn.Linear(num_ftrs, 2)
    print("ğŸš€ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù†Ù…ÙˆØ°Ø¬ Ù‚Ø¯ÙŠÙ…ØŒ Ø¬Ø§Ø±ÙŠ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ù…Ù† Ø§Ù„Ø¨Ø¯Ø§ÙŠØ©...")

# Ù†Ù‚Ù„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù„Ù‰ GPU Ø¥Ø°Ø§ ÙƒØ§Ù† Ù…ØªØ§Ø­Ù‹Ø§
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# Ø¥Ø¹Ø¯Ø§Ø¯ Ø¯Ø§Ù„Ø© Ø§Ù„ÙÙ‚Ø¯ ÙˆØ§Ù„Ù…ÙØ­Ø³Ù‘Ù†
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.0001)

# Ø¥Ø¹Ø¯Ø§Ø¯ ReduceLROnPlateau
scheduler = optim.lr_scheduler.ReduceLROnPlateau(
    optimizer, mode='min', factor=0.5, patience=3, verbose=True
)

# ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
epochs = 20
best_loss = float("inf")

for epoch in range(epochs):
    model.train()
    running_loss = 0.0

    for images, labels in dataloader:
        images, labels = images.to(device), labels.to(device)
        
        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()

    avg_loss = running_loss / len(dataloader)
    print(f"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}")
    
    # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ù€ learning rate
    scheduler.step(avg_loss)

    # Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø£ÙØ¶Ù„
    if avg_loss < best_loss:
        best_loss = avg_loss
        with open(best_model_path, "wb") as file:
            pickle.dump(model, file)
        print(f"ğŸ’¾ ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø¬Ø¯ÙŠØ¯ Ù…Ø¹ Ø£Ù‚Ù„ Ø®Ø³Ø§Ø±Ø©: {best_loss:.4f}")

print("âœ… ØªÙ… Ø§Ù„Ø§Ù†ØªÙ‡Ø§Ø¡ Ù…Ù† Ø§Ù„ØªØ¯Ø±ÙŠØ¨!")

# Ø¬Ø§Ù‡Ø² Ù„Ù„ØªØ´ØºÙŠÙ„ ÙˆØ§Ù„Ø§Ø®ØªØ¨Ø§Ø± ğŸš€
